# Mysql 实战学习笔记

## SQL查询是如何执行的

SQL中的查询语句是使用频率最高的SQL命令，当执行一次如`select * from table where id = 10`时究竟发生了什么事情呢？

通常，Mysql 分为 Server 层与存储引擎

Server 层包括连接器，查询缓存，分析器，优化器和执行器等，其涵盖了所有的内置函数（时间、数学等），并且所有的跨存储引擎的功能都在这层实现，如存储过程、触发器和视图等

存储引擎负责数据的存储和提取，其采用插件架构实现模式，支持InnoDB，MyISAM与Memory等存储引擎，从Mysql 5.5.5开始 InnoDB成为默认的存储引擎。

当然，在创建表(create table) 时候可以显示指定存储引擎: engine=InnoDB

整体结构如图所示：

![](../images/query_statement_process.png)


### 连接器

通常我们使用 Mysql Client 连接 Mysql Server，在Server中处理连接的就是连接器

`mysql -h $ip -P $port -u$ user -p`

连接器负责连接验证与鉴权工作，如果连接长期处于空闲状态，连接器会自动断开此连接，默认空闲时间为8小时，由参数wait_timeout控制

一方面，建立连接是一个复杂耗费资源的工作，所以要尽量减少连接建立，倾向于使用长连接

另一方面，mysql执行过程中临时使用的内存是管理在连接对象里面，这部分内存直到连接断开才会被释放，这又与长连接使用矛盾。

通常建议:

* 定期断开长连接
* Mysql 5.7及之后版本，每执行一次大操作后，会通过 mysql_reset_connection 重试初始化连接资源，而不需要重连


### 查询缓存

当执行查询语句时，会优选查询缓存，看是否有该查询语句的缓存记录，缓存通过key-value形式存放在内存中，key是查询语句，value是返回结果

但是，如果表数据有更新，相应的查询缓存也会失效，因此对于更新频繁的表而言，查询缓存往往弊大于利。

从 mysql 8.0 开始，查询缓存整个模块被剔除

### 分析器

如果查询缓存没有命中，则进行分析器阶段，对sql命令进行解析

分析器会把sql语句中的关键词解析出来，如table识别为对应的表，id识别为对应的属性列，这个过程称之为”词法分析“

完成之后进入到”语法分析“，即根据词法分析的结果，按照mysql的语法规则，判断sql语句是否满足mysql的语法。

### 优化器

通过分析器，mysql已经知道了sql语句要做的事情，再经过优化器来决定如何更好地完成这件事情。

例如`mysql> select * from t1 join t2 using(ID)  where t1.c=10 and t2.d=20;
`
采用先join操作再作条件过滤的方式，或者先条件过滤再join方式都能达到一样的效果，但是对应的执行效率是不一样的，优化器会判断哪种方式效率更优，从而选择更优的实现方式。

### 执行器

分析器知道了做什么事情，优化器知道了如何做这个事情更佳，最后由执行器负责完成

执行器直接对接存储引擎，**首先判断是否拥有该表的权限**，如果有则打开表执行，执行器会根据引擎提供的接口，执行得到查询结果返回

### 疑问总结

* 判断是否有对该表的权限为什么是在执行器完成，而不是之前？

很多时候，SQL语句需要操作的表并不只是SQL字面上那些，比如有个触发器，得在执行器阶段（打开表）才能确定

* 若表T没有字段T，那么执行`select * from T where k=1`时会在什么极端查到异常

分析器阶段，该阶段不仅做了词法分析与语法分析，还会做语义分析

解析器处理语法和解析查询, 生成一课对应的解析树。

预处理器进一步检查解析树的合法。比如: 数据表和数据列是否存在, 别名是否有歧义等。如果通过则生成新的解析树，再提交给优化器

* 分析器做了一次权限验证，执行器又做了一次，区别是什么？

连接器“取”权限，执行器“用”权限


## SQL更新是如何执行的

类似于SQL查询，更新语句也要经过分析器，优化器与执行器等处理流程，不同的是还需要涉及到两个重要的日志模块：redo log (重做日志) 和 binlog (归档日志)

### redo log

更新过程中存在一个问题，如果每次更新操作都要写入磁盘，则每次的更新操作都需要现在磁盘找到该条记录，然后更新，带来的IO与查找成本是相当高的

为了解决这个问题，mysql中提出了write-ahead logging的技术，即，先写日志，再写磁盘。

每当有记录需要更新时，InnoDB会先写到redo log，并更新内存，就算该次更新操作完成，而后InnoDB会在适当的时候将数据写回磁盘

有了 redo log，InnoDB 就可以保证即使数据库发生异常重启后，之前提交的记录不会丢失，这个能力称为crash-safe

### binlog

前面的redo log其实是存储引擎完成的，是引擎层面的日志，而mysql server层也有自己的日志模块，称为binlog

redo log vs. binlog

* redo log是InnoDB引擎特有的，binlog是server层实现的
* redo log是物理日志，记录在某个数据页上做了什么操作；而binlog是逻辑日志，记录的是sql语句的原始逻辑
* redo log大小固定，会循环写（覆盖）；binlog 文件达到一定大小后切换到下一个，不会覆盖

更新过程如下：

![](../images/update_statement_process.png)

注意最后三步，redo log写入过程拆成了两个步骤：prepare 和 commit，这就是**两阶段提交**

### 两阶段提交

两阶段提交存在的意义就是让两份日志之间的逻辑一致

可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的行状态不一致

### 总结

redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit这个参数设置为1的时候，每次事务的redo log都会持久化到磁盘

sync_binlog 这个参数设置成 1 的时候，表示每次事务的binlog都会持久化到磁盘



### 疑问解答

* 在两阶段过程中发生异常会如何处置？

过程： 1 prepare阶段 2 写binlog 3 commit

当在2之前崩溃时，
重启恢复：后发现没有commit，回滚。备份恢复：没有binlog 。
一致

当在3之前崩溃，
重启恢复：虽没有commit，但满足prepare和binlog完整，所以重启后会自动commit。备份：有binlog，且与redolog一致

* 写redo也是IO操作，也需要耗费性能

Redolog是顺序写，并且可以组提交，还有别的一些优化，收益最大是是这两个因素

* 只有commit完成才算成功，才能提交到数据库吗？

正常情况下需要commit结束才算是完成，但是崩溃恢复过程中是可以接受”redolog处于prepare阶段（写完），且binlog完整（写完）“的情况

* redo log 与 binlog 如何对应上？

通过事务ID

* 如何才能让数据库恢复到今天某一秒的状态

首先找到最近一次全量备份，从备份的时间点开始，取出binlog重放到指定时刻


## 数据

### 内存数据写回磁盘

执行数据更新操作时，会利用WAL机制，先写日志后写磁盘，将redo log写磁盘，就视作更新成功，那么数据是在何时被写回到磁盘呢？

当内存数据页与磁盘数据页内容不一致时，该内存页称为”脏页“，内存数据写入磁盘后，内存与磁盘数据页就一致了，这时候内存数据页就变为了”干净页“

数据写回磁盘的时机：

1. InnoDB中的redo log写满，这时会停止所有的更新操作开展数据页的flush
2. 系统内存不足，淘汰掉一些脏页来缓解内存压力
3. MySQL 认为系统当前处于空闲状态，则启动flush
4. MySQL 正常关闭时，会将所有的脏页flush到磁盘

脏页被flush回磁盘时，如果旁边的数据页也刚好是脏页，则会被连带着一起flush掉，这对于机械硬盘时代是很有意义的，通过`innodb_flush_neighbors`来控制这个行为，1代表开启，0代表关闭，从Mysql 8.0开始默认为0.

### 磁盘文件回收

数据表中的数据信息既可以存放在共享表空间，也可以存在于单独的文件，这个由参数`innodb_file_per_table`控制，值为OFF代表数据存放在系统共享表空间，值为ON代表数据存储在以.ibd为后缀的文件中

从Mysql 5.5.6版本开始，默认值为ON

表单独存储为一个文件时更容易管理，当drop table时会直接删除该文件，如果存储到共享表空间，即使表被drop掉，空间也不会被回收




## 事务隔离

事务是数据库操作的单位，一个事务内的操作要么全部成功，要么全部失败，这是事务最本质的特征

事务具备ACID特性：

* 原子性
* 一致性
* 隔离性
* 持久性

而当数据库中同时存在多个事务的时候，可能出现以下问题：

* 脏读
* 不可重复读
* 幻读

为了解决存在的这些问题，提出了事务的”隔离级别“

* 读未提交(read uncommitted) ： 一个事务还未提交时，其修改就能被其他事务看到
* 读已提交(read committed) ： 一个事务提交以后，其修改才能被其他事务看到
* 可重复读(repeatable read) ： 一个事务执行过程中读取到的数据，在整个事务执行期间都是一致的，且具备读已提交的特性
* 串行化(serializable) ： 对于同一行数据，读加读锁，写加写锁，当读写或者写写冲突时候，只能等待

在Mysql数据库中，支持多种存储引擎，但并不是所有存储引擎都支持事务，MyISAM并不支持，这是其被InnoDB替换的重要原因之一

InnoDB的默认隔离级别是可重复读，查看数据库的隔离级别： `show variables like 'transaction'`


### 事务隔离的实现

事务隔离实际上是借助于undo日志实现的，Mysql中，记录在执行更新操作同时会记录回滚操作，在新值基础上，通过回滚操作能够得到当次更新前的值

![](../images/undo_transaction_process.png)

事务启动时候，会创建当前时刻的read-view，每次读取时候是从read-view中读取，从而保证了可重复读

”创建“read-view 是依据 undo log， 实际并不是每次都真的物理创建read-view，而是依据 undo log 推算出对应更新状态下记录值

那么回滚日志什么时候被清理掉呢？ 在不被需要的时候被清理，即系统会判断不再有事务依赖于该回滚日志，也就是说当前系统没有比该回滚日志更早的read-view

### 事务的启动方式

Mysql的启动日志有两种：

* 显式启动事务，通过 begin / start transaction 启动事务， 通过 commit 结束事务， 通过 rollback 回滚
* 隐式启动事务，通过 set autocommit = 0 ，当执行如select时候便会启动事务，直到执行commit/rollback或者连接断开，事务才结束，可能导致长事务

建议使用 set autocommit = 1， 也就是显式启动事务，可以避免无意导致的长事务

查看当前存在的长事务

`select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60`



## 索引

索引存在的意义便是提升数据库查询的效率

常见的实现索引的方式：

* 哈希表，无序新增，更新速度快，查询定位也快，但是对于范围查询而言是无效的，只适用于等值查询；
* 有序数组，等值于范围查询都很快，但是更新成本大（为了保持有序）
* 二叉搜索树，查询与维护成本都比较低，但是对于树的深度较大，对于磁盘访问而言时间成本太大

综上，在数据库中，索引的实现通常使用B+树来实现，而对于Mysql而言，索引的实现由存储引擎负责，不同引擎的实现方式可能不同

### InnoDB 的索引模型

**主键索引** ： 主键索引的叶子节点是整行数据，在InnoDB中主键索引被称为聚簇索引(clustered index)
**非主键索引** ： 非主键索引的叶子节点内容是主键的值，在InnoDB中非主键索引被称为二级索引(secondary index)

通过主键索引与非主键索引进行查询的区别在于： 主键缩影查到对应的叶子节点之后可以直接拿到记录的内容，而非主键索引查到对应的叶子节点之后拿到的是主键的值，还需要再
借助主键缩影进行记录的查询，这个再次查询的过程称为”回表“，因此非主键索引会比主键索引多一次查询过程

选择怎样的字段作为主键呢？这里需要考虑到二级索引的叶子节点是主键值，所以主键值越长占用的字节数越多，则二级索引叶子节点耗费的存储空间就越多，通常我们会选择自增长ID作为
主键，这样能使得二级索引的叶子节点占据的字节空间较小，且由于其按序增长，不会产生由于中值插入导致的B+数节点分裂的问题（分裂成本比较高）

而过表中只有一个索引，且是唯一索引，那么不存在考虑二级索引的问题，可以直接使用长度较长的业务字段作为索引，以避免”回表“查询


### 普通索引 vs. 唯一索引

主键索引肯定是唯一索引，而唯一索引不一定主键索引，抛开主键索引来说，普通索引与唯一索引的区别在于值得唯一性

从查询过程看：

* 唯一索引：查询到对应的值以后，由于索引定义的唯一性，不会继续检索
* 普通索引：查询到对应的值以后，会继续检索下一个记录，直到检索到不满足查询条件的记录

查询过程中，这两种索引查询的性能开销差异微乎其微，可以认为等效

从更新过程看：

当需要更新一个数据页时候（一个数据页等同于一个B+树节点），如果数据页在内存中则直接在内存中更新，如果没在内存中，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存到change buffer 中，如此可不用从磁盘读取该数据页，当下次查询访问该数据页（读入内存）时，将 change buffer 中与该数据页相关的操作执行

change buffer 的数据也是会持久化的数据，存在于内存以及磁盘中，将 change buffer 执行到对应的数据页的操作称为 merge，除了访问数据页的操作会触发merge外，后台线程
也会定期执行merge，并且数据库正常关闭时也会执行merge

普通索引与唯一锁定的更新过程区别体现在：当更新记录的数据页不在内存中时，对于普通索引而言，可以将更新操作记录在change buffer中，更新操作就完成了；而对于唯一索引而言，需要
将该数据页读入内存中以判断是否存在冲突，而磁盘读取操作是数据库中成本最高的操作之一

但是，如果对于某些业务，执行更新后立马需要进行查询操作，也就是在访问时需要触发merge读数据页，那么change buffer是没有任何效果的，反倒是增加了维护成本

### redo log vs. change buffer

当执行语句：`insert into t(id,k) values(id1,k1),(id2,k2)`

插入两条记录到t表中，k1记录所在的数据页在内存中(InnoDB buffer pool)中，k2所在的数据页不在内存中

![](../images/redo_log_and_change_buffer.png)


那么更新语句会做如下操作：

1. Page 1 在内存中，直接更新内存中的数据页记录
2. Page 2 不在内存中，那么会在内存中的change buffer区域，记录该更新操作
3. 将上述操作(1 or 2)记录到redo log中(顺序写磁盘)

完成以上操作后，事务结束，如此可看出，更新语句的更新成本相当低，更新了两处内存，并且完成一次写磁盘操作（合并后顺序写入）

总结：

* **redo log 主要节省的是随机写磁盘的IO消耗（转为顺序写）**
* **change buffer 主要节省的是随机读磁盘的IO消耗**


### 索引选择

### 疑问解答

* change buffer 记录在内存中的数据操作记录，如果在 merge 会磁盘前发生异常重启，数据是否会丢失

对数据的操作会记录到redo log中，而对change buffer的修改操作也会记录到redo log，所以崩溃恢复的时候能够被找回来

## 锁

Mysql 中的锁分为全局锁、表级锁以及行锁

### 全局锁

全局锁是对整个数据库加锁，命令是`Flush tables with read lock`，使得整个数据库处于只读状态，变更语句都会被阻塞，其使用的典型场景是做全库逻辑备份


### 表级锁

表级锁分为两种：表锁，元数据锁

* 表锁

lock tables ... read/write，加锁后通过 unlock table 解锁或者当客户端断开时候自动释放

* 元数据锁（MDL: metadata lock）

MDL 不需要显式使用，在对表进行增删查改操作时候，自动加上MDL，即禁止此时对表结构进行变更

### 行锁

Mysql 中的行锁由存储引擎实现，并不是所有的存储引擎都支持行锁，如MyISAM就不支持，这也是其被InnoDB取代的重要原因之一。

**两阶段锁**：在InnoDB事务中，行锁是在需要的时候加上，也就是需要对记录进行变更操作时候对所在记录行加锁，但是解锁却不是在用完就进行，而是在事务结束时才会释放

由于存在上述设定，所以当事务中存在多条记录需要加锁时，建议将最有可能造成锁冲突的加锁操作（对指定记录行变更）放到最后，以减少其他事务的等待时间

#### 死锁与死锁检测

当并发系统之间的事务出现资源循环依赖，涉及到的线程都在等待其他线程释放资源，就可能导致这些线程出现无限等待的情况，这种状态称为”死锁“

当发现出现”死锁“以后，可采用以下策略加以破解：

* 设置等待超时时间，对应的参数是 `innodb_lock_wait_timeout`，默认是50s
* 发起死锁检测，发现死锁后主动回滚死锁链中的某个事务，将参数`innodb_deadlock_detect`设置为on，开启该逻辑

由于超时时间的设定过长或者过短都会对系统造成影响，所以建议采用第二种策略，即主动死锁检测，但是主动加测是会带来额外的系统开销

如何控制死锁检测带来的额外开销呢？

* 如果可以保证该业务一定不会出现死锁，可以临时将死锁检测关闭
* 控制并发度，比如同一行记录最多允许10个线程更新，那么死锁检测的成本就会很低
* 设计优化，可以将易冲突的行拆解成多行，以降低冲突几率


